# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/11_clip-for-wikiseealsotitles.ipynb.

# %% auto 0
__all__ = ['CLIP001']

# %% ../nbs/11_clip-for-wikiseealsotitles.ipynb 2
import os,torch, torch.multiprocessing as mp, pickle, numpy as np, math, transformers, scipy.sparse as sp, pandas as pd, torch.nn as nn
from tqdm.auto import tqdm
from PIL import Image

from transformers import CLIPModel, CLIPPreTrainedModel, CLIPProcessor, AutoConfig
from transformers.trainer_utils import RemoveColumnsCollator
from transformers import BatchEncoding
import torch.nn.functional as F

from xcai.basics import *
from xcai.models.modeling_utils import *
from xcai.losses import *

from xclib.utils.sparse import retain_topk

from fastcore.utils import *

# %% ../nbs/11_clip-for-wikiseealsotitles.ipynb 4
# os.environ['CUDA_VISIBLE_DEVICES'] = '6,7,8,9,10,11,12,13'
os.environ['WANDB_PROJECT']='oakVn_01-amazontitles131'

@patch
def __call__(self:RemoveColumnsCollator, features):
    if isinstance(features, list):
        features = [self._remove_columns(feature) for feature in features]
    elif isinstance(features, dict) or isinstance(features, BatchEncoding):
        features = self._remove_columns(features)
    else:
        raise ValueError(f'Invalid input type: {type(features)}')
    return self.data_collator(features)

class SelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.projection_dim % config.text_config.num_attention_heads == 0

        self.c_attn = nn.Linear(config.projection_dim, 3 * config.projection_dim)
        self.c_proj = nn.Linear(config.projection_dim, config.projection_dim)

        self.n_head = config.text_config.num_attention_heads
        self.n_embd = config.projection_dim

        block_size = 2 * config.text_config.max_position_embeddings
        self.register_buffer('bias', torch.tril(torch.ones(block_size, block_size))
                             .view(1, 1, block_size, block_size), persistent=False)

    def forward(self, x):
        B, T, C = x.size()

        qkv = self.c_attn(x)
        q, k, v = qkv.split(self.n_embd, dim=2)

        k = k.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)
        q = q.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)

        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)
        y = y.transpose(1, 2).contiguous().view(B, T, C)

        y = self.c_proj(y)
        return y

class CLIP002Encoder(CLIPModel):

    def __init__(self, config):
        super().__init__(config)
        self.attn = SelfAttention(config)
        self.post_init()

    def forward(
        self,
        data_input_ids:Optional[torch.Tensor] = None,
        data_attention_mask:Optional[torch.Tensor] = None,
        img2data_pixel_values:Optional[torch.FloatTensor] = None,
        img2data_data2ptr: Optional[torch.FloatTensor] = None,
        **kwargs
    ):
        text_outputs = self.text_model(
            input_ids=data_input_ids,
            attention_mask=data_attention_mask,
            **kwargs
        )
        text_embeds = text_outputs[1]
        text_embeds = self.text_projection(text_embeds)

        idx = torch.where(img2data_data2ptr > 0)[0]
        if len(idx) > 0: 
            bsz,n_img = len(idx),img2data_data2ptr.max()
            assert torch.all(img2data_data2ptr[idx] == n_img)

            vision_outputs = self.vision_model(
                pixel_values=img2data_pixel_values,
                **kwargs
            )
            image_embeds = vision_outputs[1]
            image_embeds = self.visual_projection(image_embeds)

            image_embeds = image_embeds.view(bsz, n_img, -1)
            embeds = torch.cat([text_embeds[idx].view(bsz, 1, -1), image_embeds], 1)
            embeds = self.attn(embeds).mean(dim=1)

            text_embeds[idx] += embeds

        return F.normalize(text_embeds, dim=-1)

class CLIP002(CLIPPreTrainedModel):
    use_generation,use_representation = False,True
    
    def __init__(self,
                 config,
                 bsz:Optional[int]=None,
                 tn_targ:Optional[int]=None,
                 margin:Optional[float]=0.3,
                 tau:Optional[float]=0.1,
                 apply_softmax:Optional[bool]=False,
                 n_negatives:Optional[int]=5,
                 use_encoder_parallel:Optional[bool]=True,
                 *args, **kwargs):
        super().__init__(config, *args, **kwargs)
        store_attr('use_encoder_parallel')
        self.encoder = CLIP002Encoder(config)
        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, n_negatives=n_negatives, tau=tau, 
                                    apply_softmax=apply_softmax, reduce='mean')
        self.post_init()

    def init_encoder(self):
        model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        ptr_sd, enc_sd = model.state_dict(), self.encoder.state_dict()
        assert len(set([k for k in enc_sd if not k.startswith('attn')]) - set(ptr_sd)) == 0, 'Pretrained model cannot initialize all the parameters'
        for k in ptr_sd:
            with torch.no_grad():
                assert enc_sd[k].shape == ptr_sd[k].shape
                enc_sd[k].copy_(ptr_sd[k])
        
    def forward(
        self,
        data_input_ids:Optional[torch.Tensor]=None,
        data_attention_mask:Optional[torch.Tensor]=None,
        lbl2data_data2ptr:Optional[torch.Tensor]=None,
        lbl2data_idx:Optional[torch.Tensor]=None,
        lbl2data_input_ids:Optional[torch.Tensor]=None,
        lbl2data_attention_mask:Optional[torch.Tensor]=None,
        plbl2data_data2ptr:Optional[torch.Tensor]=None,
        plbl2data_idx:Optional[torch.Tensor]=None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        if self.use_encoder_parallel: 
            encoder = XCDataParallel(module=self.encoder)
        else: encoder = self.encoder
        
        data_repr = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, 
                img2data_pixel_values=kwargs['img2data_pixel_values'], img2data_data2ptr=kwargs['img2data_data2ptr'])
        
        loss, lbl2data_repr = None, None
        if lbl2data_input_ids is not None:
            lbl2data_repr = encoder(data_input_ids=lbl2data_input_ids, data_attention_mask=lbl2data_attention_mask,  
                    img2data_pixel_values=kwargs['img2lbl_pixel_values'], img2data_data2ptr=kwargs['img2lbl_lbl2ptr'])
            
            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, 
                                plbl2data_data2ptr, plbl2data_idx, **kwargs)

        if not return_dict:
            o = (data_repr, lbl2data_repr)
            return ((loss,) + o) if loss is not None else o

        return XCModelOutput(
            loss=loss,
            data_repr=data_repr,
            lbl2data_repr=lbl2data_repr,
        )


# %% ../nbs/11_clip-for-wikiseealsotitles.ipynb 13
if __name__ == '__main__':
    build_block = False
    data_dir = '/data/From_B/'
    pkl_dir = '/home/aiscuser/scratch1/datasets/'
    
    output_dir = '/home/aiscuser/scratch1/outputs/mogic/12_clip-multimodal-for-amazontitles131-001'
    
    """ Load data """
    pkl_file = f'{pkl_dir}/processed/amazontitles131_data-img_openai-clip-vit-base-patch32_sxc.pkl'
    if build_block:
        block = SXCBlock.from_cfg(data_dir, 'data_img', dset='amazontitles131', tokenizer='openai/clip-vit-base-patch32', padding=True, return_tensors='pt', 
                max_sequence_length=32, n_slbl_samples=1, main_oversample=False, n_sdata_meta_samples=1, n_slbl_meta_samples=1, meta_oversample=False, 
                info_column_names=['identifier', 'file_name'], tokenization_column='file_name')

        # Load images and add it to `meta_info`
        def read_image(file):
            return Image.open(file)
        
        def read_images(files):
            with mp.Pool(processes=20) as pool:
                images = [o for o in tqdm(pool.imap(read_image, files), total=len(files))]
            return images

        data_dir = '/data/From_B/LF-AmazonTitles-131K/'
        image_dir = f'/home/aiscuser/scratch1/sugar/data/amazon_review_2023/products/images/'

        files = [f'{image_dir}/{fn}' for fn in block.train.dset.meta['img_meta'].meta_info['file_name']]
        images = read_images(files)

        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        img_info = processor(images=images, return_tensors="pt", padding=True)

        block.train.dset.meta['img_meta'].meta_info.update(img_info)
        block.test.dset.meta['img_meta'].meta_info.update(img_info)

        block.train.dset.meta['img_meta'].meta_info_keys.append('pixel_values')
        block.test.dset.meta['img_meta'].meta_info_keys.append('pixel_values')

        with open(pkl_file, 'wb') as file: pickle.dump(block, file)
        exit()
    else:
        with open(pkl_file, 'rb') as file: block = pickle.load(file)

    block.train.dset.meta['img_meta'].meta_oversample = True
    block.train.dset.meta['img_meta'].n_sdata_meta_samples = 3
    block.train.dset.meta['img_meta'].n_slbl_meta_samples = 3

    block.test.dset.meta['img_meta'].meta_oversample = True
    block.test.dset.meta['img_meta'].n_sdata_meta_samples = 3
    block.test.dset.meta['img_meta'].n_slbl_meta_samples = 3

    """ Training arguements """
    args = XCLearningArguments(
        output_dir=output_dir,
        logging_first_step=True,
        per_device_train_batch_size=400,
        per_device_eval_batch_size=400,
        representation_num_beams=200,
        representation_accumulation_steps=10,
        save_strategy="steps",
        evaluation_strategy="steps",
        eval_steps=5000,
        save_steps=5000,
        save_total_limit=5,
        num_train_epochs=300,
        predict_with_representation=True,
        adam_epsilon=1e-6,
        warmup_steps=100,
        weight_decay=0.01,
        learning_rate=2e-4,
        representation_search_type='BRUTEFORCE',
        
        output_representation_attribute='data_repr',
        label_representation_attribute='data_repr',
        metadata_representation_attribute='data_repr',
        data_augmentation_attribute='data_repr',
        representation_attribute='data_repr',
        clustering_representation_attribute='data_repr',
    
        group_by_cluster=True,
        num_clustering_warmup_epochs=10,
        num_cluster_update_epochs=5,
        num_cluster_size_update_epochs=25,
        use_data_metadata_for_clustering=True,
        clustering_type='EXPO',
        minimum_cluster_size=2,
        maximum_cluster_size=1600,

        metric_for_best_model='P@1',
        load_best_model_at_end=True,
        target_indices_key='plbl2data_idx',
        target_pointer_key='plbl2data_data2ptr',
        
        use_distributional_representation=False,
        use_encoder_parallel=True,
        max_grad_norm=None, 
        fp16=True,
        
        label_names=['img2data_pixel_values', 'img2data_data2ptr', 'img2lbl_pixel_values', 'img2lbl_lbl2ptr'],
        
        prune_metadata=False,
        num_metadata_prune_warmup_epochs=10,
        num_metadata_prune_epochs=5,
        metadata_prune_batch_size=2048,
        prune_metadata_names=['img_meta'],
        use_data_metadata_for_pruning=True,
    
        predict_with_augmentation=False,
        use_augmentation_index_representation=False,
    
        data_aug_meta_name='img',
        augmentation_num_beams=3,
        data_aug_prefix='img',
        use_label_metadata=True,
        
        data_meta_batch_size=2048,
        augment_metadata=False,
        num_metadata_augment_warmup_epochs=10,
        num_metadata_augment_epochs=5,
    
        use_cpu_for_searching=True,
        use_cpu_for_clustering=True,
    )

    """ model """
    bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()
    config = AutoConfig.from_pretrained("openai/clip-vit-base-patch32")
    model = CLIP002(config, bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, n_negatives=10, apply_softmax=True, use_encoder_parallel=True)
    model.init_encoder()
    
    # # debug
    # breakpoint()

    # batch = block.train.one_batch(10)
    # batch = prepare_batch(model, batch, m_args=['data_input_ids', 'data_attention_mask', 'img2data_pixel_values', 'img2data_data2ptr',
    #     'lbl2data_input_ids', 'lbl2data_attention_mask', 'img2lbl_pixel_values', 'img2lbl_lbl2ptr'])

    # batch = batch.to('cuda')
    # model = model.to('cuda')

    # model.use_encoder_parallel = False
    # o = model(**batch)

    # breakpoint()
    # # debug

    """ Training """
    metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                      pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])
    
    learn = XCLearner(
        model=model, 
        args=args,
        train_dataset=block.train.dset,
        eval_dataset=block.test.dset,
        data_collator=block.collator,
        compute_metrics=metric,
    )

    # print(learn.evaluate())
    learn.train()
