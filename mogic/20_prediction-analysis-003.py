# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/20_prediction-analysis.ipynb.

# %% auto 0
__all__ = ['display_momos']

# %% ../nbs/20_prediction-analysis.ipynb 2
import pandas as pd, os, pickle, numpy as np, torch, torch.nn.functional as F
from scipy import sparse
from typing import List
from tqdm.auto import tqdm
from torch.utils.data import Dataset, DataLoader
from typing import Optional

from xcai.basics import *
from xcai.analysis import *

from xclib.utils.sparse import retain_topk
from xcai.models.distillation import TCH001
from xcai.models.classifiers import CLS001

from IPython.display import HTML,display

# %% ../nbs/20_prediction-analysis.ipynb 4
def score_data_lbl(data_lbl:sparse.csr_matrix, data_repr:torch.Tensor, lbl_repr:torch.Tensor, batch_size:Optional[int]=2048):
    data_repr,lbl_repr = F.normalize(data_repr, dim=1), F.normalize(lbl_repr, dim=1)
    curr_data_lbl = data_lbl.copy()
    rows, cols = data_lbl.nonzero()
    dl = DataLoader(list(zip(rows, cols)), batch_size=batch_size, shuffle=False)
    score = None
    for b in tqdm(dl, total=len(dl)): 
        sc = data_repr[b[0]].unsqueeze(1)@lbl_repr[b[1]].unsqueeze(2)
        sc = sc.squeeze()
        score = sc if score is None else torch.hstack([score, sc])
    curr_data_lbl.data[:] = score
    curr_data_lbl.eliminate_zeros()
    return curr_data_lbl

def display_momos(momos_dset:Dataset, oak_dset:Dataset, test_dset:Dataset, idxs:List):
    df = pd.DataFrame([test_dset[i] for i in tqdm(idxs)])

    df = df.rename({'data_input_text':'Document', 'lbl2data_input_text': 'Ground truth labels', 'lnk2data_input_text':'Predicted metadata'}, axis=1)
    momos_df = pd.DataFrame({'MOMOS predictions': [momos_dset[i]['lbl2data_input_text'] for i in tqdm(idxs)]})
    oak_df = pd.DataFrame({'OAK predictions': [oak_dset[i]['lbl2data_input_text'] for i in tqdm(idxs)]})
    
    df = pd.concat([df, momos_df, oak_df], axis=1)
    return df
    
# %% ../nbs/20_prediction-analysis.ipynb 7
if __name__ == '__main__':
    topk,num_preds,metric = 5,10,'P'

    teacher_dir = '/data/Projects/xc_nlg/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-6-3'
    representation_dir = '/data/Projects/xc_nlg/outputs/85-oak-dr-ep-for-wikiseealso-with-additive-renee-embedding-5-3'

    odir_a = '/data/Projects/xc_nlg/outputs/86-distillation-for-wikiseealso-with-oak-7-3-4'
    odir_b = '/data/Projects/xc_nlg/outputs/85-oak-dr-ep-for-wikiseealso-with-additive-renee-embedding-5-3'

    os.makedirs(f'{odir_a}/examples/', exist_ok=True)
    mname = os.path.basename(odir_b)
    output_file = f'{odir_a}/examples/{mname}_examples-005.csv'

    """ Load data """
    dataset_name = 'wikiseealsotitles'
    pkl_dir = '/home/aiscuser/scratch1/datasets/'
    pkl_file = f'{pkl_dir}/processed/{dataset_name}_data-lnk_distilbert-base-uncased_xcs.pkl'
    with open(pkl_file, 'rb') as file: block = pickle.load(file)

    data_meta = retain_topk(block.test.dset.meta.lnk_meta.data_meta, k=3)
    lbl_meta = block.test.dset.meta.lnk_meta.lbl_meta
    block.test.dset.meta.lnk_meta.update_meta_matrix(data_meta, lbl_meta)

    """ Load predictions """
    pred_file = f'{odir_a}/predictions/test_predictions.pkl'
    plbl_a = Filterer.apply(get_pred_sparse(pred_file, block.n_lbl), block.test.dset.data.data_lbl_filterer)
    
    pred_file = f'{odir_b}/predictions/test_predictions.pkl'
    plbl_b = Filterer.apply(get_pred_sparse(pred_file, block.n_lbl), block.test.dset.data.data_lbl_filterer)

    """ Analysis """
    pattern = r'^(data|lbl2data|lnk2data)_input_text$'

    pdset_a = TextColumns(get_pred_dset(retain_topk(plbl_a, k=topk), block), pat=pattern)
    pdset_b = TextColumns(get_pred_dset(retain_topk(plbl_b, k=topk), block), pat=pattern)
    test_dset = TextColumns(block.test.dset, pat=pattern)

    eval_a = pointwise_eval(plbl_a, block.test.dset.data.data_lbl, block.test.dset.data.data_lbl_filterer, topk=topk, metric=metric)
    eval_b = pointwise_eval(plbl_b, block.test.dset.data.data_lbl, block.test.dset.data.data_lbl_filterer, topk=topk, metric=metric)
    eval_a = np.array(eval_a.sum(axis=1)).squeeze()
    eval_b = np.array(eval_b.sum(axis=1)).squeeze()

    """ Filtering """
    m_teacher = TCH001.from_pretrained(f'{teacher_dir}/teacher', n_data=block.train.dset.n_data, n_lbl=block.n_lbl)
    test_repr = torch.load(f'{representation_dir}/representation/test_repr.pth')

    with torch.no_grad():
        data_lbl_teacher = score_data_lbl(retain_topk(plbl_b, k=topk), test_repr, m_teacher.lbl_repr.weight, batch_size=2048)
    scores_b = np.array(data_lbl_teacher.sum(axis=1)).squeeze()

    # scores_b = np.array(retain_topk(plbl_b, k=topk).sum(axis=1)).squeeze()

    """ finding examples """
    idxs = np.argsort(eval_b - eval_a)
    idxs = idxs[:1000]

    scores_linker = np.array(data_meta.mean(axis=1)).squeeze()
    idxs = idxs[np.argsort(-scores_linker[idxs])][:100]

    oak_idxs = np.where(eval_b == 0)[0]
    idxs = np.intersect1d(idxs, oak_idxs)
    idxs = idxs[np.argsort(scores_b[idxs])]

    df = display_momos(pdset_a, pdset_b, test_dset, idxs)
    df['MOMOS scores'] = eval_a[idxs]
    df['OAK scores'] = eval_b[idxs]
    df['model scores'] = scores_b[idxs]

    df.to_csv(output_file)

    
