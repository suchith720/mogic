# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_ngame-for-wikiseealsotitles-with-input-concatenation.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/00_ngame-for-wikiseealsotitles-with-input-concatenation.ipynb 2
import os,torch,json, torch.multiprocessing as mp, pickle, numpy as np, scipy.sparse as sp
from xcai.basics import *
from xcai.models.PPP0XX import DBT009,DBT011

# %% ../nbs/00_ngame-for-wikiseealsotitles-with-input-concatenation.ipynb 4
os.environ['CUDA_VISIBLE_DEVICES'] = '6,7'
os.environ['WANDB_PROJECT']='mogic_01-msmarco'

# %% ../nbs/00_ngame-for-wikiseealsotitles-with-input-concatenation.ipynb 26
if __name__ == '__main__':
    parse_args = parse_args()

    output_dir = '/home/aiscuser/scratch1/outputs/mogic/17_ngame-for-msmarco-from-scratch-001'
    config_file, config_key = 'configs/msmarco.json', 'data'
    mname = 'distilbert-base-uncased'

    do_inference = parse_args.do_train_inference or parse_args.do_test_inference or parse_args.save_train_inference \
            or parse_args.save_test_inference or parse_args.save_repr

    """ Load data """
    pkl_file = f'{parse_args.pkl_dir}/processed/msmarco_data_distilbert-base-uncased_xcs.pkl'
    if parse_args.build_block:
        main_build_block(pkl_file, config_file, parse_args.build_block, config_key=config_key,
                tokenizer="distilbert-base-uncased", n_slbl_samples=1, main_oversample=False,
                n_sdata_meta_samples=1, n_slbl_meta_samples=1, meta_oversample=False)
    else:
        with open(pkl_file, 'rb') as file: block = pickle.load(file)

        """ Training Arguements """
        args = XCLearningArguments(
            output_dir=output_dir,
            logging_first_step=True,
            per_device_train_batch_size=800,
            per_device_eval_batch_size=800,
            representation_num_beams=200,
            representation_accumulation_steps=10,
            save_strategy="steps",
            evaluation_strategy="steps",
            eval_steps=5000,
            save_steps=5000,
            save_total_limit=5,
            num_train_epochs=300,
            predict_with_representation=True,
            representation_search_type='BRUTEFORCE',
            adam_epsilon=1e-6,
            warmup_steps=100,
            weight_decay=0.01,
            learning_rate=2e-5,
            
            group_by_cluster=True,
            num_clustering_warmup_epochs=10,
            num_cluster_update_epochs=5,
            num_cluster_size_update_epochs=25,
            clustering_type='EXPO',
            minimum_cluster_size=2,
            maximum_cluster_size=1600,
            
            metric_for_best_model='P@1',
            load_best_model_at_end=True,
            target_indices_key='plbl2data_idx',
            target_pointer_key='plbl2data_data2ptr',
            
            use_encoder_parallel=True,
            max_grad_norm=None,
            fp16=True,
        )

        metric = PrecReclMrr(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200], mk=[5, 10, 20])

        """ Model """
        def init_model(model): model.init_dr_head()
        
        def load_model(mname, bsz):
            model = DBT009.from_pretrained(mname, bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, 
                                           n_negatives=10, apply_softmax=True, use_encoder_parallel=True)
            return model

        bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()
        model = main_load_model(args.output_dir, load_model, {"mname": mname, "bsz": bsz}, init_model, do_inference=do_inference)

        learn = XCLearner(
            model=model, 
            args=args,
            train_dataset=block.train.dset,
            eval_dataset=block.test.dset,
            data_collator=block.collator,
            compute_metrics=metric,
        )

        main_run(learn, parse_args, n_lbl=block.n_lbl)


