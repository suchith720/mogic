# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_benchmarking_nvembed_bm25.ipynb.

# %% auto 0
__all__ = ['pkl_dir', 'data_dir', 'output_dir', 'pkl_file', 'block', 'input_text', 'tokenized_text', 'model', 'o', 'prompt_func',
           'RepresentationHead', 'NVM0XXEncoder', 'NVM009']

import os,torch, torch.multiprocessing as mp, pickle, numpy as np, math, transformers, torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM

from xcai.basics import *

from xclib.utils.sparse import retain_topk

from fastcore.utils import *
from fastcore.meta import *

os.environ['CUDA_VISIBLE_DEVICES'] = '14,15'
os.environ['WANDB_MODE'] = 'disabled'

import torch.nn as nn
from xcai.losses import MultiTriplet

from xcai.models.modeling_utils import XCModelOutput, Pooling

from transformers import ModernBertModel

import torch._dynamo
torch._dynamo.config.suppress_errors = True

class MBT0XXEncoder(nn.Module):
    
    def __init__(self, **kwargs):
        super().__init__()
        self.model = ModernBertModel.from_pretrained("answerdotai/ModernBERT-base") 
        
    def forward(
        self, 
        input_ids:Optional[torch.Tensor]=None, 
        attention_mask:Optional[torch.Tensor]=None,
        pool_mask: Optional[torch.Tensor]=None,
        return_dict: bool=True,
        **kwargs
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        return outputs, F.normalize(Pooling.mean_pooling(outputs[0], attention_mask), dim=1)
    

# %% ../nbs/03_benchmarking_nvembed_bm25.ipynb 36
class MBT009(nn.Module):
    use_generation,use_representation = False,True
    
    def __init__(self,
                 bsz:Optional[int]=None,
                 tn_targ:Optional[int]=None,
                 margin:Optional[float]=0.3,
                 tau:Optional[float]=0.1,
                 apply_softmax:Optional[bool]=False,
                 n_negatives:Optional[int]=5,
                 use_encoder_parallel:Optional[bool]=True,
                 *args, **kwargs):
        super().__init__(*args, **kwargs)
        store_attr('use_encoder_parallel')
        self.encoder = MBT0XXEncoder()
        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, n_negatives=n_negatives, tau=tau, 
                                    apply_softmax=apply_softmax, reduce='mean')
        
    def forward(
        self,
        data_input_ids:Optional[torch.Tensor]=None,
        data_attention_mask:Optional[torch.Tensor]=None,
        lbl2data_data2ptr:Optional[torch.Tensor]=None,
        lbl2data_idx:Optional[torch.Tensor]=None,
        lbl2data_input_ids:Optional[torch.Tensor]=None,
        lbl2data_attention_mask:Optional[torch.Tensor]=None,
        plbl2data_data2ptr:Optional[torch.Tensor]=None,
        plbl2data_idx:Optional[torch.Tensor]=None,
        return_dict: Optional[bool] = None,
        **kwargs
    ):
        data_o, data_repr = self.encoder(data_input_ids, data_attention_mask)
        
        loss, lbl2data_repr = None, None
        if lbl2data_input_ids is not None:
            lbl2data_o, lbl2data_repr = self.encoder(lbl2data_input_ids, lbl2data_attention_mask)
            
            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, 
                                plbl2data_data2ptr, plbl2data_idx, **kwargs)

        return XCModelOutput(
            loss=loss,
            data_repr=data_repr,
            lbl2data_repr=lbl2data_repr,
        )

if __name__ == '__main__':
    build_block = False
    data_dir = '/data/datasets/'
    pkl_dir = '/home/aiscuser/scratch1/datasets/'
    
    pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data_modern-bert_xcs.pkl'
    if build_block:
        block = XCBlock.from_cfg('data', data_dir, transform_type='xcs', tokenizer="answerdotai/ModernBERT-base",
                                 sampling_features=[('lbl2data',1)], oversample=True)
        with open(pkl_file, 'wb') as file: pickle.dump(block, file)
        exit()
    else:
        with open(pkl_file, 'rb') as file: block = pickle.load(file)
    
    model = MBT009(bsz=1024, margin=0.3, tau=0.1, n_negatives=10, apply_softmax=True, use_encoder_parallel=False)
    
    batch = next(iter(block.train.dl))
    batch = prepare_batch(model, batch)

    model, batch = model.to('cuda'), batch.to('cuda')

    with torch.no_grad():
        o = model(**batch)

    print(o.loss)

