{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60b51bd-0144-4139-8be5-7602bad6a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 03_benchmarking_nvembed_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e00e5c-ff88-425d-a828-7ca5d02215ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874750be-c904-447e-8754-3eefcb9586d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os,torch, torch.multiprocessing as mp, pickle, numpy as np, math, transformers\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from xcai.basics import *\n",
    "\n",
    "from xclib.utils.sparse import retain_topk\n",
    "\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd23053-8908-4615-a47f-96b2039b0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44292259-cc09-4bd0-96f3-08206b948924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['WANDB_PROJECT']='oakVn_00-wikiseealsotitles'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7055b-7bcc-4c55-8bac-c9312cb56d23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Huggingface `NV-Embed-v2` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9735f180-8557-4bb7-9670-499b74e617b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name_to_instruct = {\"example\": \"Given a question, retrieve passages that answer the question\",}\n",
    "\n",
    "query_prefix = \"Instruct: \"+task_name_to_instruct[\"example\"]+\"\\nQuery: \"\n",
    "queries = [\n",
    "    'are judo throws allowed in wrestling?', \n",
    "    'how to become a radiology technician in michigan?'\n",
    "    ]\n",
    "\n",
    "passage_prefix = \"\"\n",
    "passages = [\n",
    "    \"Since you're reading this, you are probably someone from a judo background or someone who is just wondering how judo techniques can be applied under wrestling rules. So without further ado, let's get to the question. Are Judo throws allowed in wrestling? Yes, judo throws are allowed in freestyle and folkstyle wrestling. You only need to be careful to follow the slam rules when executing judo throws. In wrestling, a slam is lifting and returning an opponent to the mat with unnecessary force.\",\n",
    "    \"Below are the basic steps to becoming a radiologic technologist in Michigan:Earn a high school diploma. As with most careers in health care, a high school education is the first step to finding entry-level employment. Taking classes in math and science, such as anatomy, biology, chemistry, physiology, and physics, can help prepare students for their college studies and future careers.Earn an associate degree. Entry-level radiologic positions typically require at least an Associate of Applied Science. Before enrolling in one of these degree programs, students should make sure it has been properly accredited by the Joint Review Committee on Education in Radiologic Technology (JRCERT).Get licensed or certified in the state of Michigan.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01a5c56d-8cdd-430d-b237-0e2f56faf1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68bace3aba44523b9fae6ba86c7a62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6658d681-3b9d-4660-a80f-3d715949f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scai/phd/aiz218323/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/5130cf1daf847c1bacee854a6ef1ca939e747fb2/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n",
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg_2/lib/python3.9/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_length = 32768\n",
    "query_embeddings = model.encode(queries, instruction=query_prefix, max_length=max_length)\n",
    "passage_embeddings = model.encode(passages, instruction=passage_prefix, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abe5ed36-45c4-408e-9f4f-a988229c455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87.42693328857422, 0.46283310651779175], [0.9652641415596008, 86.0372085571289]]\n"
     ]
    }
   ],
   "source": [
    "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e69af54-2aac-4031-aa49-c4ab36bc56ea",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258d66e0-92d6-4c4d-9edc-06eb30fbf040",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "\n",
    "output_dir = '/home/scai/phd/aiz218323/scratch/outputs/mogic/03_benchmarking_nvembed_bm25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cccd8075-3756-4c71-bcba-f339143a1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nvidia/NV-Embed-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bec817-b538-41b4-b6c7-72102ea71b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a14386bf-ac0f-4aef-8df2-ac1f92daa75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data_distilbert-base-uncased_xcs.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef77a0b-5d20-40eb-9781-b8f690fc7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data_nv-embed-v2_xcs.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3879382e-324b-4215-8d04-e9b8a7842d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = XCBlock.from_cfg(data_dir, 'data', transform_type='xcs', tokenizer='nvidia/NV-Embed-v2', \n",
    "                         sampling_features=[('lbl2data',1)], max_sequence_length=64, oversample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85abc8b-0245-4ee3-9c0d-aa13427dd323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d042d5a-600c-4ffa-a647-f715a9465667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_func(x):\n",
    "    return f'''Instruct: Given the title of a wikipedia article and the corresponding categories of that article on wikipedia, \\\n",
    "your task is to predict the titles of all articles which are likely to be listed in the see also section of the mentioned article.\\\n",
    "\\nQuery: {x}'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f87b7b1-ca0a-44f5-bf51-c01baed909de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_func(x):\n",
    "    return f'''Instruct: Given the title of a wikipedia article, your task is to predict the titles of all articles which are \\\n",
    "likely to be listed in the see also section of the mentioned article.\\nQuery: {x}'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf3cff8-fbd2-41d9-81a0-c6cb0fbef537",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [prompt_func(o) for o in block.train.dset.data.data_info['input_text']]\n",
    "tokenized_text = tokenizer.batch_encode_plus(input_text, truncation=True, max_length=64)\n",
    "block.train.dset.data.data_info.update(tokenized_text)\n",
    "\n",
    "input_text = [prompt_func(o) for o in block.test.dset.data.data_info['input_text']]\n",
    "tokenized_text = tokenizer.batch_encode_plus(input_text, truncation=True, max_length=64)\n",
    "block.test.dset.data.data_info.update(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633821e8-af3c-47d8-8643-82c00cb8c9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c8e7b8c-b672-4629-9fad-9a310735bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f9f188f-8cdd-4340-aeef-f7a70b1f6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59ae3d-3f62-4f5e-8534-643ca9492d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7428b36f-55fa-4183-8211-80ddaa8cd89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(block.train.dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a611b67-966c-4f4b-a0a7-fbc559a6cb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'data_idx'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004d2d7-1bb0-4077-84ed-d03e77bb7e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fd6532a8-9b98-40b2-9f61-d96a161df3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca11573e5e3449fb832e86989923bace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = AutoModel.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e4866ce8-c774-47f4-83e4-5740d853e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg_2/lib/python3.9/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "o = m(**{'input_ids': batch['data_input_ids'], 'attention_mask': batch['data_attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4bcfd6c4-a4ec-4793-8ab2-f8584d3e5c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 47, 4096])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o['sentence_embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78025432-5290-41be-980a-705aad176290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 47])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['data_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c87285-5a57-4ee0-82ba-f1dfbadb4c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653ee19c-38b0-48e4-8613-54b8d447cccf",
   "metadata": {},
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee3fca-4766-4033-8a26-f8a73983705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prompt_func(x):\n",
    "    return f'''Instruct: Given the title of a wikipedia article, your task is to predict the titles of all articles which are \\\n",
    "likely to be listed in the see also section of the mentioned article.\\nQuery: {x}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67be8d-d2b0-40d2-a772-8d068ad99a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='10396500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [      21/10396500 10:23 < 94728:46:30, 0.03 it/s, Epoch 0.00/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@10</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@10</th>\n",
       "      <th>N@3</th>\n",
       "      <th>N@5</th>\n",
       "      <th>Psp@1</th>\n",
       "      <th>Psp@10</th>\n",
       "      <th>Psp@3</th>\n",
       "      <th>Psp@5</th>\n",
       "      <th>Psn@1</th>\n",
       "      <th>Psn@10</th>\n",
       "      <th>Psn@3</th>\n",
       "      <th>Psn@5</th>\n",
       "      <th>R@200</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.086276</td>\n",
       "      <td>0.175101</td>\n",
       "      <td>0.056812</td>\n",
       "      <td>0.115025</td>\n",
       "      <td>0.087132</td>\n",
       "      <td>0.175101</td>\n",
       "      <td>0.194849</td>\n",
       "      <td>0.173305</td>\n",
       "      <td>0.180583</td>\n",
       "      <td>0.163741</td>\n",
       "      <td>0.209324</td>\n",
       "      <td>0.168619</td>\n",
       "      <td>0.180018</td>\n",
       "      <td>0.163741</td>\n",
       "      <td>0.201765</td>\n",
       "      <td>0.174354</td>\n",
       "      <td>0.185467</td>\n",
       "      <td>0.429889</td>\n",
       "      <td>0.235065</td>\n",
       "      <td>0.384685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3840' max='8876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3840/8876 03:31 < 04:38, 18.11 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9dddc6d9a64058b82b2ca61ae6b27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e7320ceba542bc8a85e6e26205e3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    build_block = True\n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "    data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "    \n",
    "    output_dir = '/home/scai/phd/aiz218323/scratch/outputs/mogic/03_benchmarking_nvembed_bm25'\n",
    "\n",
    "    \"\"\" Load data \"\"\"\n",
    "    pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data_nv-embed-v2_xcs.pkl'\n",
    "\n",
    "    if build_block:\n",
    "        block = XCBlock.from_cfg(data_dir, 'data', transform_type='xcs', tokenizer='nvidia/NV-Embed-v2', \n",
    "                                 sampling_features=[('lbl2data',1)], max_sequence_length=64, oversample=False)\n",
    "\n",
    "        input_text = [prompt_func(o) for o in block.train.dset.data.data_info['input_text']]\n",
    "        tokenized_text = tokenizer.batch_encode_plus(input_text, truncation=True, max_length=64)\n",
    "        block.train.dset.data.data_info.update(tokenized_text)\n",
    "        \n",
    "        input_text = [prompt_func(o) for o in block.test.dset.data.data_info['input_text']]\n",
    "        tokenized_text = tokenizer.batch_encode_plus(input_text, truncation=True, max_length=64)\n",
    "        block.test.dset.data.data_info.update(tokenized_text)\n",
    "\n",
    "        with open(pkl_file, 'wb') as file: pickle.dump(block, file)\n",
    "        exit()\n",
    "    else:\n",
    "        with open(pkl_file, 'rb') as file: block = pickle.load(file)\n",
    "    \n",
    "\n",
    "    \"\"\" Training arguements \"\"\"\n",
    "    args = XCLearningArguments(\n",
    "        output_dir=output_dir,\n",
    "        logging_first_step=True,\n",
    "        per_device_train_batch_size=800,\n",
    "        per_device_eval_batch_size=800,\n",
    "        representation_num_beams=200,\n",
    "        representation_accumulation_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=5000,\n",
    "        save_steps=5000,\n",
    "        save_total_limit=5,\n",
    "        num_train_epochs=300,\n",
    "        predict_with_representation=True,\n",
    "        adam_epsilon=1e-6,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=2e-4,\n",
    "        representation_search_type='BRUTEFORCE',\n",
    "        \n",
    "        output_representation_attribute='data_repr',\n",
    "        label_representation_attribute='data_repr',\n",
    "        metadata_representation_attribute='data_repr',\n",
    "        data_augmentation_attribute='data_repr',\n",
    "        representation_attribute='data_repr',\n",
    "        clustering_representation_attribute='data_repr',\n",
    "    \n",
    "        group_by_cluster=True,\n",
    "        num_clustering_warmup_epochs=10,\n",
    "        num_cluster_update_epochs=5,\n",
    "        num_cluster_size_update_epochs=25,\n",
    "        use_data_metadata_for_clustering=True,\n",
    "        clustering_type='EXPO',\n",
    "        minimum_cluster_size=2,\n",
    "        maximum_cluster_size=1600,\n",
    "\n",
    "        metric_for_best_model='P@1',\n",
    "        load_best_model_at_end=True,\n",
    "        target_indices_key='plbl2data_idx',\n",
    "        target_pointer_key='plbl2data_data2ptr',\n",
    "        \n",
    "        use_distributional_representation=False,\n",
    "        use_encoder_parallel=True,\n",
    "        max_grad_norm=None, \n",
    "        fp16=True,\n",
    "        \n",
    "        label_names=['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_attention_mask'],\n",
    "    \n",
    "        predict_with_augmentation=False,\n",
    "        use_augmentation_index_representation=True,\n",
    "    \n",
    "        use_cpu_for_searching=False,\n",
    "        use_cpu_for_clustering=True,\n",
    "    )\n",
    "\n",
    "    \"\"\" model \"\"\"\n",
    "    bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "    model = NVM009.from_pretrained('nvidia/NV-Embed-v2', bsz=bsz, margin=0.3, tau=0.1, n_negatives=10, apply_softmax=True, \n",
    "                                   use_encoder_parallel=False)\n",
    "    \n",
    "    model.init_dr_head()\n",
    "    \n",
    "    \"\"\" Training \"\"\"\n",
    "    metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                      pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])\n",
    "\n",
    "    learn = XCLearner(\n",
    "        model=model, \n",
    "        args=args,\n",
    "        train_dataset=block.train.dset,\n",
    "        eval_dataset=block.test.dset,\n",
    "        data_collator=block.collator,\n",
    "        compute_metrics=metric,\n",
    "    )\n",
    "    \n",
    "    mp.freeze_support()\n",
    "    learn.train()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
